{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Analyse Exploratoire des Données (EDA)\n",
    "\n",
    "**Objectifs :**\n",
    "1. Comprendre le périmètre et la qualité des données\n",
    "2. Calculer les KPIs business (justifier le besoin de segmentation)\n",
    "3. Explorer les features RFM et décider des transformations\n",
    "4. Documenter les décisions pour le preprocessing\n",
    "\n",
    "**Dataset :** UCI Online Retail (541 909 transactions, 8 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins robustes\n",
    "ROOT = Path.cwd()\n",
    "if not (ROOT / \"data\" / \"raw\" / \"Online Retail.xlsx\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "DATA_PATH = ROOT / \"data\" / \"raw\" / \"Online Retail.xlsx\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"figures\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ROOT: {ROOT}\")\n",
    "print(f\"DATA_PATH exists: {DATA_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Chargement et contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Mémoire: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperçu\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau récapitulatif contexte\n",
    "context = {\n",
    "    \"Période\": f\"{df['InvoiceDate'].min().date()} → {df['InvoiceDate'].max().date()}\",\n",
    "    \"Durée (jours)\": (df['InvoiceDate'].max() - df['InvoiceDate'].min()).days,\n",
    "    \"Transactions\": f\"{len(df):,}\",\n",
    "    \"Factures uniques\": f\"{df['InvoiceNo'].nunique():,}\",\n",
    "    \"Clients uniques\": f\"{df['CustomerID'].nunique():,}\",\n",
    "    \"Produits uniques\": f\"{df['StockCode'].nunique():,}\",\n",
    "    \"Pays\": df['Country'].nunique(),\n",
    "    \"Pays principal\": f\"{df['Country'].value_counts().index[0]} ({df['Country'].value_counts().iloc[0]/len(df)*100:.1f}%)\"\n",
    "}\n",
    "pd.DataFrame.from_dict(context, orient='index', columns=['Valeur'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Qualité des données (décisions nettoyage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs manquantes\n",
    "missing = pd.DataFrame({\n",
    "    'Manquants': df.isnull().sum(),\n",
    "    '%': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "}).sort_values('%', ascending=False)\n",
    "missing[missing['Manquants'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomalies\n",
    "anomalies = {\n",
    "    \"CustomerID null\": (df['CustomerID'].isnull().sum(), df['CustomerID'].isnull().mean() * 100),\n",
    "    \"Quantity <= 0\": ((df['Quantity'] <= 0).sum(), (df['Quantity'] <= 0).mean() * 100),\n",
    "    \"UnitPrice <= 0\": ((df['UnitPrice'] <= 0).sum(), (df['UnitPrice'] <= 0).mean() * 100),\n",
    "    \"Factures 'C' (annulées)\": (df['InvoiceNo'].astype(str).str.startswith('C').sum(), \n",
    "                                 df['InvoiceNo'].astype(str).str.startswith('C').mean() * 100)\n",
    "}\n",
    "pd.DataFrame.from_dict(anomalies, orient='index', columns=['Count', '%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation valeurs manquantes\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "missing_plot = missing[missing['Manquants'] > 0]\n",
    "ax.barh(missing_plot.index, missing_plot['%'], color='coral')\n",
    "ax.set_xlabel('% manquant')\n",
    "ax.set_title('Valeurs manquantes')\n",
    "for i, v in enumerate(missing_plot['%']):\n",
    "    ax.text(v + 0.5, i, f'{v:.1f}%', va='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"01_missing_values.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### DÉCISIONS NETTOYAGE\n\n| Anomalie | Action | Justification |\n|----------|--------|---------------|\n| CustomerID null (~25%) | **Supprimer** | Obligatoire pour RFM |\n| Quantity ≤ 0 (~2%) | **Exclure** | Retours/annulations |\n| UnitPrice ≤ 0 (~0.4%) | **Exclure** | Prix invalides |\n| Factures 'C' | **Exclure** | Annulations explicites |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. KPIs Business\n",
    "\n",
    "**Objectif** : Quantifier la valeur et justifier le besoin de segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Données valides pour KPIs (hors anomalies)\ndf_valid = df[\n    (df['CustomerID'].notna()) & \n    (df['Quantity'] > 0) & \n    (df['UnitPrice'] > 0) &\n    (~df['InvoiceNo'].astype(str).str.startswith('C'))  # Exclure annulations\n].copy()\n\ndf_valid['TotalAmount'] = df_valid['Quantity'] * df_valid['UnitPrice']\nprint(f\"Transactions valides: {len(df_valid):,} ({len(df_valid)/len(df)*100:.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPIs globaux\n",
    "kpis = {\n",
    "    \"Revenue total (£)\": f\"{df_valid['TotalAmount'].sum():,.0f}\",\n",
    "    \"Nb commandes\": f\"{df_valid['InvoiceNo'].nunique():,}\",\n",
    "    \"Nb clients\": f\"{df_valid['CustomerID'].nunique():,}\",\n",
    "    \"AOV (Average Order Value)\": f\"£{df_valid.groupby('InvoiceNo')['TotalAmount'].sum().mean():,.2f}\",\n",
    "    \"Panier moyen par ligne\": f\"£{df_valid['TotalAmount'].mean():,.2f}\",\n",
    "    \"Revenue/client moyen\": f\"£{df_valid.groupby('CustomerID')['TotalAmount'].sum().mean():,.2f}\"\n",
    "}\n",
    "pd.DataFrame.from_dict(kpis, orient='index', columns=['Valeur'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue par pays (Top 5)\n",
    "revenue_country = df_valid.groupby('Country')['TotalAmount'].sum().sort_values(ascending=False)\n",
    "top5 = revenue_country.head(5)\n",
    "top5_pct = (top5 / revenue_country.sum() * 100).round(1)\n",
    "\n",
    "print(\"Top 5 pays par revenue:\")\n",
    "pd.DataFrame({'Revenue (£)': top5.apply(lambda x: f\"{x:,.0f}\"), '% total': top5_pct})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARETO : Concentration de valeur\n",
    "customer_revenue = df_valid.groupby('CustomerID')['TotalAmount'].sum().sort_values(ascending=False)\n",
    "customer_revenue_cumsum = customer_revenue.cumsum() / customer_revenue.sum() * 100\n",
    "\n",
    "# Trouver le % de clients pour 80% du CA\n",
    "n_customers = len(customer_revenue)\n",
    "for pct in [10, 20, 30]:\n",
    "    n_top = int(n_customers * pct / 100)\n",
    "    revenue_pct = customer_revenue.head(n_top).sum() / customer_revenue.sum() * 100\n",
    "    print(f\"Top {pct}% clients ({n_top:,}) = {revenue_pct:.1f}% du CA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation Pareto\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = np.arange(1, len(customer_revenue) + 1) / len(customer_revenue) * 100\n",
    "y = customer_revenue.cumsum() / customer_revenue.sum() * 100\n",
    "\n",
    "ax.plot(x, y, 'b-', linewidth=2)\n",
    "ax.axhline(80, color='red', linestyle='--', alpha=0.7, label='80% du CA')\n",
    "ax.axvline(20, color='green', linestyle='--', alpha=0.7, label='20% des clients')\n",
    "ax.fill_between(x, y, alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('% cumulé des clients (triés par revenue décroissant)')\n",
    "ax.set_ylabel('% cumulé du CA')\n",
    "ax.set_title('Courbe de Pareto - Concentration de valeur client')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"01_pareto_curve.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSIGHT PARETO\n",
    "\n",
    "**~20% des clients génèrent ~X% du CA** → Forte concentration de valeur.\n",
    "\n",
    "**Implication** : La segmentation RFM permettra d'identifier ces clients à haute valeur (\"Champions\") vs clients à risque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. RFM Exploratoire\n",
    "\n",
    "**Objectif** : Calculer R/F/M provisoires et analyser leurs distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date de référence = max + 1 jour\n",
    "reference_date = df_valid['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
    "print(f\"Date de référence: {reference_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul RFM\n",
    "rfm = df_valid.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': lambda x: (reference_date - x.max()).days,  # Recency\n",
    "    'InvoiceNo': 'nunique',  # Frequency\n",
    "    'TotalAmount': 'sum'  # Monetary\n",
    "}).reset_index()\n",
    "\n",
    "rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
    "print(f\"Shape RFM: {rfm.shape}\")\n",
    "rfm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats descriptives RFM\n",
    "rfm[['Recency', 'Frequency', 'Monetary']].describe(percentiles=[0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributions RFM\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "# Row 1: Histogrammes\n",
    "for ax, col, color in zip(axes[0], ['Recency', 'Frequency', 'Monetary'], ['#3498db', '#2ecc71', '#e74c3c']):\n",
    "    ax.hist(rfm[col], bins=50, color=color, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(rfm[col].median(), color='black', linestyle='--', label=f'Médiane: {rfm[col].median():.0f}')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Clients')\n",
    "    ax.set_title(f'Distribution {col}')\n",
    "    ax.legend()\n",
    "\n",
    "# Row 2: Boxplots\n",
    "for ax, col, color in zip(axes[1], ['Recency', 'Frequency', 'Monetary'], ['#3498db', '#2ecc71', '#e74c3c']):\n",
    "    bp = ax.boxplot(rfm[col], vert=True, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor(color)\n",
    "    ax.set_ylabel(col)\n",
    "    ax.set_title(f'Boxplot {col} (outliers visibles)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"01_rfm_distributions.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détection outliers (IQR)\n",
    "def detect_outliers_iqr(series):\n",
    "    Q1, Q3 = series.quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = ((series < lower) | (series > upper)).sum()\n",
    "    return outliers, outliers / len(series) * 100\n",
    "\n",
    "print(\"Outliers (méthode IQR):\")\n",
    "for col in ['Recency', 'Frequency', 'Monetary']:\n",
    "    n_out, pct_out = detect_outliers_iqr(rfm[col])\n",
    "    print(f\"  {col}: {n_out:,} outliers ({pct_out:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness (asymétrie)\n",
    "print(\"Skewness (asymétrie):\")\n",
    "for col in ['Recency', 'Frequency', 'Monetary']:\n",
    "    skew = rfm[col].skew()\n",
    "    print(f\"  {col}: {skew:.2f} {'(fortement asymétrique)' if abs(skew) > 1 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test log-transform sur Frequency et Monetary\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for ax, col in zip(axes, ['Frequency', 'Monetary']):\n",
    "    log_values = np.log1p(rfm[col])  # log(1+x) pour gérer les 0\n",
    "    ax.hist(log_values, bins=50, color='purple', edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel(f'log(1 + {col})')\n",
    "    ax.set_ylabel('Clients')\n",
    "    ax.set_title(f'Distribution log-transformée {col} (skew: {log_values.skew():.2f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"01_rfm_log_transform.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DÉCISION TRANSFORMATIONS\n",
    "\n",
    "| Feature | Skewness | Outliers | Décision |\n",
    "|---------|----------|----------|----------|\n",
    "| Recency | Modéré | ~X% | RobustScaler |\n",
    "| Frequency | Fort | ~X% | Log-transform + RobustScaler |\n",
    "| Monetary | Fort | ~X% | Log-transform + RobustScaler |\n",
    "\n",
    "**Justification** : Distributions très skewed → log-transform réduit l'asymétrie, RobustScaler gère les outliers restants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Validation préliminaire du scaling\n",
    "\n",
    "**Objectif** : Comparer rapidement les approches de scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation features\n",
    "X_raw = rfm[['Recency', 'Frequency', 'Monetary']].values\n",
    "X_log = np.column_stack([\n",
    "    rfm['Recency'].values,\n",
    "    np.log1p(rfm['Frequency'].values),\n",
    "    np.log1p(rfm['Monetary'].values)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison scaling (K=4 fixe pour test rapide)\n",
    "K_TEST = 4\n",
    "results = []\n",
    "\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler()\n",
    "}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    # Sans log\n",
    "    X_scaled = scaler.fit_transform(X_raw)\n",
    "    km = KMeans(n_clusters=K_TEST, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_scaled)\n",
    "    sil = silhouette_score(X_scaled, labels)\n",
    "    results.append({'Méthode': name, 'Log': 'Non', 'Silhouette': sil})\n",
    "    \n",
    "    # Avec log\n",
    "    X_scaled_log = scaler.fit_transform(X_log)\n",
    "    labels_log = km.fit_predict(X_scaled_log)\n",
    "    sil_log = silhouette_score(X_scaled_log, labels_log)\n",
    "    results.append({'Méthode': name, 'Log': 'Oui', 'Silhouette': sil_log})\n",
    "\n",
    "results_df = pd.DataFrame(results).pivot(index='Méthode', columns='Log', values='Silhouette')\n",
    "results_df = results_df[['Non', 'Oui']]  # Réordonner colonnes\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparaison\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "results_df.plot(kind='bar', ax=ax, color=['#e74c3c', '#2ecc71'])\n",
    "ax.set_ylabel('Score Silhouette (K=4)')\n",
    "ax.set_title('Comparaison Scaling : Impact du log-transform')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "ax.legend(title='Log-transform')\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"01_scaling_comparison.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DÉCISION SCALING\n",
    "\n",
    "**Choix retenu** : `Log-transform (F, M)` + `RobustScaler`\n",
    "\n",
    "**Justification** :\n",
    "- Log-transform améliore le score silhouette\n",
    "- RobustScaler résiste aux outliers résiduels\n",
    "- Combinaison robuste pour données e-commerce (distributions skewed typiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Synthèse EDA → Décisions\n\n### Règles de nettoyage\n\n| # | Règle | Impact |\n|---|-------|--------|\n| 1 | Supprimer `CustomerID` null | -25% lignes |\n| 2 | Exclure `Quantity ≤ 0` | -2% lignes |\n| 3 | Exclure `UnitPrice ≤ 0` | -0.4% lignes |\n| 4 | Exclure factures `C` (annulées) | Inclus dans #2 |\n\n### Feature engineering\n\n| # | Action |\n|---|--------|\n| 5 | Créer `TotalAmount = Quantity × UnitPrice` |\n| 6 | Log-transform sur Frequency et Monetary |\n| 7 | RobustScaler sur R, log(F), log(M) |\n\n### Insights business\n\n| Métrique | Valeur |\n|----------|--------|\n| Concentration Pareto | ~20% clients = ~X% CA |\n| AOV | £X |\n| Clients segmentables | ~4,300 |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation finale\n",
    "print(\"=== ESTIMATION POST-PREPROCESSING ===\")\n",
    "print(f\"Lignes conservées: {len(df_valid):,} / {len(df):,} ({len(df_valid)/len(df)*100:.1f}%)\")\n",
    "print(f\"Clients pour clustering: {rfm.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**EDA VALIDÉ** → Prochaine étape : `02_preprocessing.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}